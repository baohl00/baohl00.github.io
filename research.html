
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bao's Research Page</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Bao H. Le</h1>
        <p>Bachelor of <br>Ho Chi Minh University of Science - Vietnam National University</p>
    <h3><a href="https://baohl00.github.io/index.html">Home</a></h3>
        <h3><a href="https://baohl00.github.io/research.html">Research</a></h3>
        <h3><a href="https://baohl00.github.io/personal.html">Personal</a></h3>
    <b>Social</b><br>
        <div class="social-row">
          <a href="mailto:lehoangbao411@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
          <a href="https://scholar.google.com/citations?user=NZh4CQoAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
          <a href="https://github.com/baohl00"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
          <br>
        </div>
        <br>

    <p><b>Contact:</b><br>Faculty of Mathematics and Computer Science<br>Ho Chi Minh University of Science<br>227 Nguyen Van Cu Street, Ward 4, District 5<br>Ho Chi Minh City, Vietnam</p>
    <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>

      </header>
      <section>

    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Published Papers</h2>
    <p style="margin:0"> <a style="https://www.sciencedirect.com/science/article/pii/S1566253518308893">Information fusion in visual question answering: A Survey</a> <br> with <a href="https://scholar.google.com.sg/citations?user=nYN9A3IAAAAJ&hl=en">Dongxiang Zhang</a> and <a href="https://scholar.google.com/citations?user=RMaqDKAAAAAJ&hl=zh-CN">Sai Wu</a> <br> <i>Journal of Information Fusion</i>, Accepted, 2019. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Visual question answering automatically answers natural language questions according to the content of an image or video. The task is challenging because it requires the understanding of semantic information in the textual and visual channels, as well as their interplay. A typical solver is composed of three components: feature extraction from singular modality, feature fusion between visual and textual channels, and answer prediction based on the learnt joint representation. Among them, information fusion plays a key role in enhancing the overall accuracy and various types of approaches have been proposed, such as simple vector operators, deep neural networks, bilinear pooling, attention mechanisms, and memory networks. The primary objective of this survey is to provide a clear organization and comprehensive review on the ever-proposed fusion techniques in the domain of visual question answering. We propose an abstract fusion framework that can fit the majority of existing VQA models, making it convenient for readers to quickly understand their key contributions. Finally, we summarize the effective fusion strategies that have been widely adopted so as to benefit readers in their model design.    
    </p></div>

    <p style="margin:0"> <a style="https://www.aclweb.org/anthology/2020.coling-main.557/">HateGAN: Adversarial Generative-Based Data Augmentation for Hate Speech Detection</a> <br> with <a href="https://scholar.google.com.sg/citations?user=uQxdOlsAAAAJ&hl=en">Roy Ka-Wei Lee</a> <br> <i>COLING</i>, Accepted, 2020. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Academia and industry have developed machine learning and natural language processing models to detect online hate speech automatically. However, most of these existing methods adopt a supervised approach that heavily depends on labeled datasets for training. This results in the methodsâ€™ poor detection performance of the hate speech class as the training datasets are highly imbalanced. In this paper, we propose HateGAN, a deep generative reinforcement learning model, which addresses the challenge of imbalance class by augmenting the dataset with hateful tweets. We conduct extensive experiments to augment two commonly-used hate speech detection datasets with the HateGAN generated tweets. Our experiment results show that HateGAN improves the detection performance of the hate speech class regardless of the classifiers and datasets used in the detection task. Specifically, we observe an average 5% improvement for the hate class F1 scores across all state-of-the-art hate speech classifiers. We also conduct case studies to empirically examine the HateGAN generated hate speeches and show that the generated tweets are diverse, coherent, and relevant to hate speech detection. 
    </p></div>
          
    <p style="margin:0"> <a style="https://arxiv.org/abs/2006.13507">On Analyzing Annotation Consistency in Online Abusive Behavior Datasets</a> <br> with <a href="https://scholar.google.com.sg/citations?user=uQxdOlsAAAAJ&hl=en">Roy Ka-Wei Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Awal%2C+M+R">Md Rabiul Awal</a> and <a href="https://scholar.google.be/citations?user=SBQZHb0AAAAJ&hl=en">Sandra Mitrovic</a> <br> <i>ICWSM</i>, Accepted, 2020. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Online abusive behavior is an important issue that breaks the cohesiveness of online social communities and even raises public safety concerns in our societies. Motivated by this rising issue, researchers have proposed, collected, and annotated online abusive content datasets. These datasets play a critical role in facilitating the research on online hate speech and abusive behaviors. However, the annotation of such datasets is a difficult task; it is often contentious on what should be the true label of a given text as the semantic difference of the labels may be blurred (e.g., abusive and hate) and often subjective. In this study, we proposed an analytical framework to study the annotation consistency in online hate and abusive content datasets. We applied our proposed framework to evaluate the consistency of the annotation in three popular datasets that are widely used in online hate speech and abusive behavior studies. We found that there is still a substantial amount of annotation inconsistency in the existing datasets, particularly when the labels are semantically similar.
    </p></div>
          
          
    <p style="margin:0"> <a style="https://ieeexplore.ieee.org/abstract/document/9311693">Action-Centric Relation Transformer Network for Video Question Answering</a> <br> with <a href="https://scholar.google.com/citations?user=q0De288AAAAJ&hl=zh-CN">Jipeng Zhang</a>, <a href="https://scholar.google.com/citations?user=ikbw5okAAAAJ&hl=en">Jie Shao</a> <br> <i>IEEE Transactions on Circuits and Systems for Video Technology </i>, Accepted, 2020. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Video question answering (VideoQA) has emerged as a popular research topic in recent years. Enormous efforts have been devoted to developing more effective fusion strategies and better intra-modal feature preparation. To explore these issues further, we identify two key problems. (1) Current works take almost no account of introducing action of interest in video representation. Additionally, there exists insufficient labeling data on where the action of interest is in many datasets. However, questions in VideoQA are usually action-centric. (2) Frame-to-frame relations, which can provide useful temporal attributes (e.g., state transition, action counting), lack relevant research. Based on these observations, we propose an action-centric relation transformer network (ACRTransformer) for VideoQA and make two significant improvements. (1) We explicitly consider the action recognition problem and present a visual feature encoding technique, action-based encoding (ABE), to emphasize the frames with high actionness probabilities (the probability that the frame has actions). (2) We better exploit the interplays between temporal frames using a relation transformer network (RTransformer). Experiments on popular benchmark datasets in VideoQA clearly establish our superiority over previous state-of-the-art models. Code could be found at https://github.com/op-multimodal/ACRTransformer.
    </p></div>
          
    <p style="margin:0"> <a style="https://dl.acm.org/doi/10.1145/3394231.3397890">DeepHate: Hate Speech Detection via Multi-Faceted Text Representations</a> <br> with <a href="https://scholar.google.com.sg/citations?user=uQxdOlsAAAAJ&hl=en">Roy Ka-Wei Lee</a> and <a href="https://scholar.google.com/citations?user=dCqnisMAAAAJ&hl=zh-CN">Tuan-Anh Hoang</a> <br> <i>ACM Conference on Web Science</i>, Accepted, 2020. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Online hate speech is an important issue that breaks the cohesiveness of online social communities and even raises public safety concerns in our societies. Motivated by this rising issue, researchers have developed many traditional machine learning and deep learning methods to detect hate speech in online social platforms automatically. However, most of these methods have only considered single type textual feature, e.g., term frequency, or using word embeddings. Such approaches neglect the other rich textual information that could be utilized to improve hate speech detection. In this paper, we propose DeepHate, a novel deep learning model that combines multi-faceted text representations such as word embeddings, sentiments, and topical information, to detect hate speech in online social platforms. We conduct extensive experiments and evaluate DeepHate on three large publicly available real-world datasets. Our experiment results show that DeepHate outperforms the state-of-the-art baselines on the hate speech detection task. We also perform case studies to provide insights into the salient features that best aid in detecting hate speech in online social platforms.  
    </p></div>
          
   <p style="margin:0"> <a style="https://dl.acm.org/doi/10.1145/3394231.3397890">Disentangling Hate in Online Memes</a> <br> with Ziqin Fan, <a href="https://scholar.google.com.sg/citations?user=uQxdOlsAAAAJ&hl=en">Roy Ka-Wei Lee, Wen-Haw Chong and <a href="https://scholar.google.com.sg/citations?user=hVTK2YwAAAAJ&hl=en">Jing Jiang</a> <br> <i>ACM Conference on Multi Media (ACM MM)</i>, Accepted, 2021. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Hateful and o ensive content detection has been extensively explored in a single modality such as text. However, such toxic information could also be communicated via multimodal content such as online memes. Therefore, detecting multimodal hateful content has recently garnered much attention in academic and industry research communities. This paper aims to contribute to this emerging research topic by proposing DisMultiHate, which is a novel framework that performed the classi cation of multimodal hateful content. Speci cally, DisMultiHate is designed to disentangle hateful attributes and target entities in multimodal memes to improve the hateful content classi cation and explainability. We conduct extensive experiments on two publicly available hateful and o ensive memes datasets. Our experiment results show that DisMultiHate is able to outperform state-of-the-art unimodal and multimodal baselines in the hateful meme classi cation task. Empirical case studies were also conducted to demonstrate DisMultiHateâ€™s ability to disentangle hateful attributes and target entities in memes and ul- timately showcase DisMultiHateâ€™s explainability of the multimodal hateful content classi cation task.  
    </p></div>
    <hr>

    <h2><a id="recent-papers-updated" class="anchor" href="#workingpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Working Papers</h2>
    
     <p style="margin:0"> <a style="https://arxiv.org/abs/2007.10712">On Analyzing Antisocial Behaviors Amid COVID-19 Pandemic</a> <br> with <a href="https://scholar.google.com.sg/citations?user=uQxdOlsAAAAJ&hl=en">Roy Ka-Wei Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Awal%2C+M+R">Md Rabiul Awal</a> and <a href="https://scholar.google.be/citations?user=SBQZHb0AAAAJ&hl=en">Sandra Mitrovic</a> <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    The COVID-19 pandemic has developed to be more than a bio-crisis as global news has reported a sharp rise in xenophobia and discrimination in both online and offline communities. Such toxic behaviors take a heavy toll on society, especially during these daunting times. Despite the gravity of the issue, very few studies have studied online antisocial behaviors amid the COVID-19 pandemic. In this paper, we fill the research gap by collecting and annotating a large dataset of over 40 million COVID-19 related tweets. Specially, we propose an annotation framework to annotate the antisocial behavior tweets automatically. We
also conduct an empirical analysis of our annotated dataset and found that new abusive lexicons are introduced amid the COVID-19 pandemic. Our study also identified the vulnerable targets of antisocial behaviors and the factors that influence the spreading of online antisocial content.
    </p></div>
          
      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
