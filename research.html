
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bao's Research Page</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Hoang-Bao Le</h1>
        <p>PhD Candidate of School of Computing - Dublin City University and The ADAPT Centre.</p>
    <h3><a href="https://baohl00.github.io">Home</a></h3>
        <h3><a href="https://baohl00.github.io/research.html">Research</a></h3>
        <h3><a href="https://baohl00.github.io/personal.html">Personal</a></h3>
        <h3><a href="https://baohl00.github.io/cv.html">CV</a></h3>
    <b>Social</b><br>
        <div class="social-row">
          <a href="mailto:bao.le2@mail.dcu.ie" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>&#13;&#10;
          <a href="https://scholar.google.com/citations?user=NZh4CQoAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
          <a href="https://github.com/baohl00"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
          <a href="https://twitter.com/baohl00"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
          <a href="https://www.linkedin.com/in/bao-l-386463140/"><i class="fa fa-fw fa-linkedin-square"></i> Linkedin</a><br>
<!--           <a href="https://www.researchgate.net/profile/Bao-Le-52"><i class="ai ai-fw ai-research-gate-square"></i> ResearchGate</a><br> -->
          <br>
        </div>
        <br>
    <p><b>Contact:</b><br>Dublin City University<br>Dublin City University, Collins Ave Ext, Whitehall, Dublin 9<br>Dublin, Ireland</p>
    <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </header>
      <section>

    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Papers</h2>
    <p style="margin:0"> <a style="https://ieeexplore.ieee.org/document/9701519">Named Entity Recognition for Vietnamese Real Estate Advertisements</a><br>Son Huynh, Khiem Le, Nhi Dang, <b>Bao Le</b>, Dang Huynh, Binh T. Nguyen, Trung T. Nguyen, Nhi Y. T. Ho <br> <i>The 8th NAFOSTED Conference on Information and Computer Science (NICS)</i>, Accepted, 2021. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    With the booming development of the Internet and e-Commerce, advertising has appeared in almost all areas of life, especially in the real estate domain. Understanding these advertising posts is necessary to capture the status of real estate transactions and rent and sale prices in different areas with various properties. Motivated by that, we present the first manually annotated Vietnamese dataset in the real estate domain. Remarkably, our dataset is annotated for the named entity recognition task with lots of entity types. In comparison to other Vietnamese NER datasets, our dataset contains the largest number of entities. We empirically investigate a strong baseline on our dataset using the API supported by the spaCy library, which comprises four main components: tokenization, embedding, encoding, and parsing. For the encoding, we conduct experiments with various encoders, including Convolutions with Maxout activation (MaxoutWindowEncoder), Convolutions with Mish activation (MishWindowEncoder), and bidirectional Long short-term memory (BiLSTMEncoder). The experimental results show that the MishWindowEncoder gives the best performance in terms of micro F1-score (90.72 %). Finally, we aim to publish our dataset later to contribute to the current research community related to named entity recognition.    </p></div>

<!--     <p style="margin:0"> <a style="https://aclanthology.org/2022.paclic-1.71/">SEND: A Simple and Efficient Noise Detection Algorithm for Vietnamese Real Estate Posts</a> <br> Khanh Quoc Tran, An Tran-Hoai Le, An Trong Nguyen, Tung Tran Nguyen Doan, Son Huynh Thanh, <b>Bao Le Hoang</b>, Hoang Nguyen Minh, Triet Minh Thai, Hoang Le Huy, Dang T. Huynh, Binh T. Nguyen, Nhi Y. T. Ho, Trung T. Nguyen <br> <i>The 36th Pacific Asia Conference on Language, Information and Computation</i>, Accepted, 2022. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    One of the emerging research fields in Natural Language Processing is Noise Detection (ND), the process of identifying posts containing noise information on textual data. While numerous datasets and approaches are developed for ND research in other languages, equivalent resources for the Vietnamese are limited. To the best of our knowledge, no dataset or method has been investigated or proposed to address the noise Detection tasks in the Vietnamese language. In reality, noise data is constantly present in datasets and sometimes hurts relevant model performance. To overcome this limitation, we propose ViND, a first human-annotated dataset that is available to the scientific community as a benchmark for the task of {\bf Vi}etnamese {\bf N}oise {\bf D}etection. The ViND dataset contains 12,862 posts collected from five major Vietnamese real estate news websites. This paper provides an overview of the Vietnamese Noise Detection task, the process of creating the ViND dataset, and the techniques for carrying out the baseline experiments. On the ViND dataset, the PhoBERT$_{large}$ model outperforms robust baseline models such as LSTM, Bi-LSTM, BERT, RoBERTa, XLM-R, and DistilBERT and achieves a macro F1-score of 0.9024. In addition, our proposed method also successfully improves the related task's performance, mainly Vietnamese Named Entity Recognition (NER) for real estate posts, about 0.0239 in terms of macro F1-score.
    </p></div> -->
          
    <p style="margin:0"> <a style="https://www.researchgate.net/publication/365644494_A_New_Approach_for_Vietnamese_Aspect-Based_Sentiment_Analysis">A New Approach for Vietnamese Aspect-Based Sentiment Analysis</a> <br> <b>Bao H. Le </b>, Hoang Minh Nguyen, Nhi Kieu-Phuong Nguyen, Binh Nguyen <br> <i>The 14th International Conference on Knowledge and Systems Engineering (KSE)</i>, Accepted, 2022. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    Intelligent systems, especially smartphones, have become crucial parts of the world. These devices can solve various human tasks, from long-distance communication to healthcare assistants. For this tremendous success, customer feedback on a smartphone plays an integral role during the development process. This paper presents an improved approach for the Vietnamese Smartphone Feedback Dataset (UIT-ViSFD), collected and annotated carefully in 2021 (including 11,122 comments and their labels) by employing the pretrained PhoBERT model with a proper pre-processing method. In the experiments, we compare the approach with other transformer-based models such as XLM-R, DistilBERT, RoBERTa, and BERT. The experimental results show that the proposed method can bypass the state-of-the-art methods related to the UIT-ViSFD corpus. As a result, our model can achieve better macro-F1 scores for the Aspect and Sentiment Detection task, which are 86.03\% and 78.76\% , respectively. In addition, our approach could improve the results of Aspect-Based Sentiment Analysis datasets in the Vietnamese language.
    </p></div>
          
          
<!--     <p style="margin:0"> <a style="https://www.researchgate.net/publication/372466158_Principal_Components_Analysis_Based_Imputation_for_Logistic_Regression">Principal Components Analysis Based Imputation for Logistic Regression</a> <br> Thuong H. T. Nguyen, <b>Bao Le</b>, Phuc Nguyen, Linh G. H. Tran, Thu Nguyen, Binh T. Nguyen <br> <i>The 36th International Conference on Industrial, Engineering & Other Applications of Applied Intelligent Systems (IEA/AIE) </i>, Accepted, 2023. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    The field of AI and machine learning is constantly evolving, and as the size of data continues to grow, so does the need for accurate and efficient methods of data processing. However, the data is not always perfect, and missing data is becoming common and occurs more frequently. Therefore, imputation techniques, aside from precision, needed to be scalable. For that reason, we examine the performance of Principle Components Analysis Imputation (PCAI), an imputation speeding up framework, for logistic regression. Note that the coefficients of a logistic regression model are usually used for interpretation. Therefore, in addition to examining the improvement in the speed of PCAI, we examine how the coefficients of fitted logistic regression models change when using this imputation speeding-up mechanism. To demonstrate the efficiency of the mentioned method, the modelâ€™s performance is compared against frequently used imputation methods on three popular datasets: Fashion MNIST, Gene, and Parkinson and achieves lower time and better accuracy in most experiments.
    </p></div> -->
          
<!--     <p style="margin:0"> <a style="">Faster Imputation Using Singular Value Decomposition for Sparse Data</a> <br> Phuc Nguyen, Linh G. H. Tran, <b>Bao H. Le</b>, Thuong H. T. Nguyen, Thu Nguyen, Hien D. Nguyen, Binh T. Nguyen  <br> <i>The 15th Asian Conference on Intelligent Information and Database Systems (AIIDS)</i>, Accepted, 2023. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    With the emergence of many knowledge-based systems worldwide, there have been more and more applications using different kinds of data and solving significant daily problems. Among that, the issues of missing data in such systems have become more popular, especially in data-driven areas. Other research on the imputation problem has dealt with partial and missing data. This study aims to investigate the imputation techniques for sparse data using the Singular Value Decomposition technique, namely SVDI. We explore the application of the SVDI framework for image classification and text classification tasks that involve sparse data. The experimental results show that the proposed SVDI method improves the speed and accuracy of the imputation process when compared to the PCAI method. We aim to publish our codes related to the SVDI later for the relevant research community.  
    </p></div> -->
          
<!--    <p style="margin:0"> <a style="https://dl.acm.org/doi/10.1145/3394231.3397890">A Deep Learning Approach to Segment High-Content Images of the E.coli Bacteria</a> <br> Dat Q. Duong, Tuan-Anh Tran, Phuong Nhi Nguyen Kieu, Tien K. Nguyen, <b>Bao Le</b>, Stephen Baker, Binh T.Nguyen <br> <i>The Advanced Concepts for Intelligent Vision Systems Conference (ACIVS)</i>, Accepted, 2023. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    High-content imaging (HCI) has been used to study antimicrobial resistance in bacteria. Although cell segmentation is a critical step for accurately analyzing bacterial populations, existing HCI platforms were not optimized for bacterial cells. This study proposes a convolutional neural network-based approach utilizing transfer learning and fine-tuning to perform instance segmentation on fluorescence images of \textit{E. coli}. The method uses the pre-trained EfficientNet as the encoder for feature extraction and U-Net for reconstructing the segmentation maps containing the cell cytoplasm and the cell instance boundary. Next, individual cells are separated using a marker-controlled watershed transformation. The EffNetB7-UNet yields the best performance with the highest F1-Score of 0.91 compared to other methods.
    </p></div> -->

    <p style="margin:0"> <a style="https://www.researchgate.net/publication/375434193_OphNER_Named_Entity_Recognition_for_Ophthalmology_Newspapers">OphNER: Named Entity Recognition for Ophthalmology Newspapers</a> <br> with <a></a></a> <br><b>Bao H. Le </b>, Thi Quynh Pham, Anh Thi Van Hoang, Binh Nguyen <br> <i>The 15th International Conference on Knowledge and Systems Engineering (KSE)</i>, Accepted, 2023. <br><button class="accordion"> 
    Abstract   
    </button>   
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> 
    The Fourth Industrial Revolution has turned electronic devices into the main tools in human daily life. At the same time, the consequences of the developed economy, such as environmental pollution, viruses, and bacterial strains, are also the main causes of eye diseases. Under the development of media, eye diseases have been quickly and fully synthesized through different types of texts. Texts on ophthalmology provide information about symptoms, disease manifestations, agents, prevention, or treatment in great detail. A large amount of information makes it more difficult to find and filter. Since then, it has become more urgent to build both a corpus and a system to identify and categorize the information from official sources so that everyone can easily find relevant information and better understand related terms to ophthalmology. One of the systems to search for information related to keywords is named entity recognition (NER). To help address this problem, we release the OphNER (<i>Oph</i>thalmology <i>NER</i>) dataset - the first corpus containing nearly 9,000 sentences with more than a total of 17,447 labels of 16 entities. We also conduct experiments with state-of-the-art models. The highest result belongs to RoBERTa_large, which is better than XLM-R_large or XLNet_large. Our dataset is released on <a href=https://github.com/baohl00/OphNER>Github</a>.
    </p></div>
          
      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
